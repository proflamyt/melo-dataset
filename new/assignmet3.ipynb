{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d1e68439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import scipy.sparse\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "af2f14a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "list(iris.keys())\n",
    "X = iris[\"data\"] # petal width\n",
    "Y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ea1a1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalize(X, norm='l2')\n",
    "def oneHotIt(Y):\n",
    "    m = Y.shape[0]\n",
    "    OHX = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "    OHX = np.array(OHX.todense()).T\n",
    "    return OHX\n",
    "y = oneHotIt(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "af147c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X[0:140]\n",
    "y_train = y[0:140]\n",
    "x_test = X[140:]\n",
    "y_test = y[140:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f3c9a12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, epochs = 10):\n",
    "        self.epochs = epochs\n",
    "        self.theta = np.random.rand(4,3)\n",
    "        self.eta = 0.1 # learning rate\n",
    "           #m = y.shape[0]\n",
    "\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        m = y.shape[0]\n",
    "        for iteration in range(self.epochs):\n",
    "            Sig_inp = X.dot(self.theta)\n",
    "            H_theta = 1/(1 + np.exp(-Sig_inp))\n",
    "            err = H_theta - y\n",
    "            SE = err * err\n",
    "            MSE = np.mean(SE)\n",
    "           # print(\"MSE after:\", iteration, \"iterations\", MSE)\n",
    "            gradients = 2/m * X.T.dot(err)\n",
    "            self.theta = self.theta - self.eta * (gradients)\n",
    "           \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        theta_best = self.theta\n",
    "        y_predict = X.dot(theta_best)\n",
    "        y_predict = 1/(1 + np.exp(-y_predict))\n",
    "        return y_predict\n",
    "    \n",
    "    \n",
    "    def error(self, y_predict, y):\n",
    "        error = np.square(np.subtract(y,y_predict)).mean() # root mean square error\n",
    "        return error\n",
    "    \n",
    "    \n",
    "    def accuracy(self, y_predict, y):\n",
    "        accuracy = sum(y_predict == y) / (float(len(y)))\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "76beaead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    \"\"\"\n",
    "    Class representing a softmax regression model.\n",
    "    Capable of performing multiclass classfication.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_iter : float, default=3000\n",
    "        Maximum number of iterations to be used by batch gradient descent.\n",
    "    lr : float, default=1e-1\n",
    "        Learning rate determining the size of steps in batch gradient descent.\n",
    "    Attributes \n",
    "    ----------\n",
    "    coef_ : array of shape (n_features,)\n",
    "        Estimated coefficients of each feature and intercept.  \n",
    "    \"\"\"\n",
    "    def __init__(self, epochs=3000, eta=1e-1):\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.theta  = np.random.rand(5,1)\n",
    "        \n",
    "    def accuracy(self, y_predict, y):\n",
    "    \n",
    "        # calculating the prediction accuracy\n",
    "        accuracy = sum(y_predict == y) / (float(len(y)))\n",
    "\n",
    "        return accuracy\n",
    "    def error(self,y_predict, y):\n",
    "        error = np.square(np.subtract(y,y_predict)).mean() # root mean square error\n",
    "        return error\n",
    "\n",
    "    def train(self, X_train_b1, y):\n",
    "        m = y.shape[0] + 1\n",
    "        for iteration in range(self.epochs):\n",
    "            Sig_inp = X_train_b1.dot(self.theta)\n",
    "            H_theta = 1 / (1 + np.exp(-Sig_inp))   # Sigmoidal function, Sig_inp = X.m or X.Theta\n",
    "            err = H_theta - y\n",
    "            errm = err * err\n",
    "            mean_err = np.mean(errm)\n",
    "            # print(\"MSE after:\", iteration, \"iterations\", mean_err)\n",
    "            gradients = 2 / m * X_train_b1.T.dot(err)\n",
    "            self.theta = self.theta - self.eta * (gradients)           # updating the weights or downhill steps (recall the downhill problem)\n",
    "\n",
    "    def predict(self, X_train_b1):\n",
    "        theta_best = self.theta\n",
    "        y_predict = X_train_b1.dot(theta_best)\n",
    "        return y_predict\n",
    "\n",
    "    def predict_proba(self, y_predict):\n",
    "        for i in range(y_predict.shape[0]):\n",
    "            for j in range(y_predict.shape[1]):\n",
    "\n",
    "                if y_predict[i][j]<0.5:\n",
    "                    y_predict[i][j]=0\n",
    "                else:\n",
    "                    y_predict[i][j]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0e91026f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 4)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3eecb60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2586208494209372"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = LogisticRegression()\n",
    "ola.train(x_train,y_train)\n",
    "y_predict = ola.predict(x_train)\n",
    "ola.error(y_predict, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bdb307",
   "metadata": {},
   "source": [
    "# Compute Error After 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fa6fe70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18191364601880983"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = LogisticRegression(epochs=100)\n",
    "ola.train(x_train,y_train)\n",
    "y_predict = ola.predict(x_train)\n",
    "ola.error(y_predict, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c1ac9",
   "metadata": {},
   "source": [
    "* Error After 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d1932167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.111780631627806"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = LogisticRegression(epochs=1000)\n",
    "ola.train(x_train,y_train)\n",
    "y_predict = ola.predict(x_train)\n",
    "ola.error(y_predict, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f4a69a",
   "metadata": {},
   "source": [
    "* Error After 10000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9fce3b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0819231719675751"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = LogisticRegression(epochs=10000)\n",
    "ola.train(x_train,y_train)\n",
    "y_predict = ola.predict(x_train)\n",
    "ola.error(y_predict, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2819b4",
   "metadata": {},
   "source": [
    "# Testing For testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3830994f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11240303334148066"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_predict = ola.predict(x_test)\n",
    "ola.error(y_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "29243d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00241908, 0.48117451, 0.75233955],\n",
       "       [0.00606408, 0.45591903, 0.50584442],\n",
       "       [0.00190391, 0.51638422, 0.78285856],\n",
       "       [0.00211104, 0.49800726, 0.77165902],\n",
       "       [0.00257418, 0.44271169, 0.76378448],\n",
       "       [0.00396876, 0.47483326, 0.62205585],\n",
       "       [0.00256062, 0.5776372 , 0.66488322],\n",
       "       [0.00416097, 0.48737999, 0.59288538],\n",
       "       [0.00349225, 0.38451313, 0.73043203],\n",
       "       [0.00346628, 0.46163961, 0.66815414]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01a2aaf",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ec13b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "list(iris.keys())\n",
    "X = iris[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "882268b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2: Extract the training data\n",
    "X_train =X[0:99] # we have used the first 100 samples. first 50 belongs to class 1 and next 50 belongs to class 2\n",
    "\n",
    "#3: Split the data into training and testing\n",
    "X_train =X[0:89]   # trainig data: here we want to use first 90 samples for training so that we can use the last 10 samples for testing\n",
    "X_test = X[90:99] # testing data\n",
    "\n",
    "#4. Normalizing the data\n",
    "X_train = normalize(X_train, norm='l2')\n",
    "X_train = X_train.T\n",
    "X_test = normalize(X_test, norm='l2')\n",
    "X_test = X_test.T\n",
    "\n",
    "\n",
    "#5. Extract the output from the dataset\n",
    "y = iris[\"target\"]\n",
    "y_train=y[0:89]\n",
    "y_test = y[90:99] \n",
    "\n",
    "#6. Reshaping the output\n",
    "y_train=np.reshape(y_train, (1, 89)) #\n",
    "y_train=y_train.T\n",
    "y_test = np.reshape(y_test, (1, 9))\n",
    "y_test = y_test.T\n",
    "\n",
    "# 7. Normalizing the output\n",
    "y_train = normalize(y_train, norm='l2')\n",
    "y_test = normalize(y_test, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "17195e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Adding bias to the model\n",
    "ones = np.ones((1, 89))\n",
    "X_train_b1 = np.append(ones, X_train, axis=0)\n",
    "X_train_b1 = X_train_b1.T\n",
    "zeros = np.ones((1,9))\n",
    "X_test_b1 = np.append(zeros, X_test, axis=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "05d0503a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43820224719101125"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = SoftmaxRegression(epochs=10)\n",
    "ola.train(X_train_b1,y_train)\n",
    "y_pred = ola.predict(X_train_b1)\n",
    "ola.predict_proba(y_pred)\n",
    "ola.error(y_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a6421198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43820224719101125"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = SoftmaxRegression(epochs=100)\n",
    "ola.train(X_train_b1,y_train)\n",
    "y_pred = ola.predict(X_train_b1)\n",
    "ola.predict_proba(y_pred)\n",
    "ola.error(y_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f444aa13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = SoftmaxRegression(epochs=1000)\n",
    "ola.train(X_train_b1,y_train)\n",
    "y_pred = ola.predict(X_train_b1)\n",
    "ola.predict_proba(y_pred)\n",
    "ola.error(y_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "dac88a42",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,9) and (5,1) not aligned: 9 (dim 1) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6326/1834100909.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mola\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSoftmaxRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mola\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mola\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mola\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mola\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6326/3054306704.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train_b1, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mSig_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_b1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mH_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mSig_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Sigmoidal function, Sig_inp = X.m or X.Theta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH_theta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,9) and (5,1) not aligned: 9 (dim 1) != 5 (dim 0)"
     ]
    }
   ],
   "source": [
    "ola = SoftmaxRegression(epochs=1000)\n",
    "ola.train(X_test,y_test)\n",
    "y_pred = ola.predict(X_test)\n",
    "ola.predict_proba(y_pred)\n",
    "ola.error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "22ef76bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 1)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86662952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
