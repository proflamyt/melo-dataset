{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "223a78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import scipy.sparse\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d47d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "list(iris.keys())\n",
    "X = iris[\"data\"] # petal width\n",
    "Y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da8e29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalize(X, norm='l2')\n",
    "def oneHotIt(Y):\n",
    "    m = Y.shape[0]\n",
    "    OHX = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "    OHX = np.array(OHX.todense()).T\n",
    "    return OHX\n",
    "y = oneHotIt(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef49e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X[0:140]\n",
    "y_train = y[0:140]\n",
    "x_test = X[140:]\n",
    "y_test = y[140:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45eb2e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, epochs = 10):\n",
    "        self.epochs = epochs\n",
    "        self.theta = np.random.rand(4,3)\n",
    "        self.eta = 0.1 # learning rate\n",
    "           #m = y.shape[0]\n",
    "\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        m = y.shape[0]\n",
    "        for iteration in range(self.epochs):\n",
    "            Sig_inp = X.dot(self.theta)\n",
    "            H_theta = 1/(1 + np.exp(-Sig_inp))\n",
    "            err = H_theta - y\n",
    "            SE = err * err\n",
    "            MSE = np.mean(SE)\n",
    "           # print(\"MSE after:\", iteration, \"iterations\", MSE)\n",
    "            gradients = 2/m * X.T.dot(err)\n",
    "            self.theta = self.theta - self.eta * (gradients)\n",
    "           \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        theta_best = self.theta\n",
    "        y_predict = X.dot(theta_best)\n",
    "        y_predict = 1/(1 + np.exp(-y_predict))\n",
    "        return y_predict\n",
    "    \n",
    "    \n",
    "    def error(self, y_predict, y):\n",
    "        error = np.square(np.subtract(y,y_predict)).mean() # root mean square error\n",
    "        return error\n",
    "    \n",
    "    \n",
    "    def accuracy(self, y_predict, y):\n",
    "        accuracy = sum(y_predict == y) / (float(len(y)))\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2dce1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    \"\"\"\n",
    "    Class representing a softmax regression model.\n",
    "    Capable of performing multiclass classfication.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_iter : float, default=3000\n",
    "        Maximum number of iterations to be used by batch gradient descent.\n",
    "    lr : float, default=1e-1\n",
    "        Learning rate determining the size of steps in batch gradient descent.\n",
    "    Attributes \n",
    "    ----------\n",
    "    coef_ : array of shape (n_features,)\n",
    "        Estimated coefficients of each feature and intercept.  \n",
    "    \"\"\"\n",
    "    def __init__(self, epochs=3000, eta=1e-1):\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.theta  = np.random.rand(5,1)\n",
    "        \n",
    "    def accuracy(self, y_predict, y):\n",
    "    \n",
    "        # calculating the prediction accuracy\n",
    "        accuracy = sum(y_predict == y) / (float(len(y)))\n",
    "\n",
    "        return accuracy\n",
    "    def error(self,y_predict, y):\n",
    "        error = np.square(np.subtract(y,y_predict)).mean() # root mean square error\n",
    "        return error\n",
    "\n",
    "    def train(self, X_train_b1, y):\n",
    "        m = y.shape[0] + 1\n",
    "        for iteration in range(self.epochs):\n",
    "            Sig_inp = X_train_b1.dot(self.theta)\n",
    "            H_theta = 1 / (1 + np.exp(-Sig_inp))   # Sigmoidal function, Sig_inp = X.m or X.Theta\n",
    "            err = H_theta - y\n",
    "            errm = err * err\n",
    "            mean_err = np.mean(errm)\n",
    "            # print(\"MSE after:\", iteration, \"iterations\", mean_err)\n",
    "            gradients = 2 / m * X_train_b1.T.dot(err)\n",
    "            self.theta = self.theta - self.eta * (gradients)           # updating the weights or downhill steps (recall the downhill problem)\n",
    "\n",
    "    def predict(self, X_train_b1):\n",
    "        theta_best = self.theta\n",
    "        y_predict = X_train_b1.dot(theta_best)\n",
    "        return y_predict\n",
    "\n",
    "    def predict_proba(self, y_predict):\n",
    "        for i in range(y_predict.shape[0]):\n",
    "            for j in range(y_predict.shape[1]):\n",
    "\n",
    "                if y_predict[i][j]<0.5:\n",
    "                    y_predict[i][j]=0\n",
    "                else:\n",
    "                    y_predict[i][j]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ac6956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3acefe67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28262016846670224"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = LogisticRegression()\n",
    "ola.train(x_train,y_train)\n",
    "y_predict = ola.predict(x_train)\n",
    "ola.error(y_predict, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dac880",
   "metadata": {},
   "source": [
    "### Predicting With Gradient Descent \n",
    "gradient descent calculates the wieghts (m) by finding the more efficient weights that can minimize the errors calculated at each turn therefore producing a better prediction . with each iteration gradient descent tries to find a weight that brings the predicted outcome closer to the actual outcome .\n",
    "\n",
    "* logistic regression is a widely used algorithm for classification, it uses a sigmoid function to calculate an output between 1 and 0 . its an algorithm best used in cases of classifications\n",
    "\n",
    "* Linear regression find the predicted outcome by using the line of best fit method, it uses the calculated weight and bias to predict a linear extention of the trained data (i.e the linear relationship between the independent variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97414d3",
   "metadata": {},
   "source": [
    "## Computing Error For multiple iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2cf390b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.183625053187671"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = LogisticRegression(epochs=100)\n",
    "ola.train(x_train,y_train)\n",
    "y_predict = ola.predict(x_train)\n",
    "ola.error(y_predict, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85087bff",
   "metadata": {},
   "source": [
    "* Error After 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "294e18b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1120165124208923"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = LogisticRegression(epochs=1000)\n",
    "ola.train(x_train,y_train)\n",
    "y_predict = ola.predict(x_train)\n",
    "ola.error(y_predict, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f455da",
   "metadata": {},
   "source": [
    "* Error After 10000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92ea58d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0815384359270191"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = LogisticRegression(epochs=10000)\n",
    "ola.train(x_train,y_train)\n",
    "y_predict = ola.predict(x_train)\n",
    "ola.error(y_predict, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c963ba",
   "metadata": {},
   "source": [
    "## Testing For testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1121f2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10988848047719155"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_predict = ola.predict(x_test)\n",
    "ola.error(y_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f79e65d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00250748, 0.47145658, 0.75320026],\n",
       "       [0.00628087, 0.44683757, 0.50905818],\n",
       "       [0.0019469 , 0.51114336, 0.78243834],\n",
       "       [0.0021695 , 0.49099058, 0.77144771],\n",
       "       [0.00268538, 0.43043427, 0.76386173],\n",
       "       [0.00410978, 0.46578383, 0.62456665],\n",
       "       [0.00259583, 0.57677227, 0.66773116],\n",
       "       [0.00425435, 0.48231632, 0.59357206],\n",
       "       [0.00365314, 0.37092823, 0.72857725],\n",
       "       [0.00353866, 0.45619113, 0.66599687]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36fd563",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1212b3",
   "metadata": {},
   "source": [
    "### Import iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17deec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "list(iris.keys())\n",
    "X = iris[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249148ca",
   "metadata": {},
   "source": [
    "### Diffrrence Between Softmax Regression and Logistic Regression\n",
    "Softmax Regression handles multiple classification cases while logistic regression is designed for binary classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c73c8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2: Extract the training data\n",
    "X_train =X[0:99] # we have used the first 100 samples. first 50 belongs to class 1 and next 50 belongs to class 2\n",
    "\n",
    "#3: Split the data into training and testing\n",
    "X_train =X[0:89]   # trainig data: here we want to use first 90 samples for training so that we can use the last 10 samples for testing\n",
    "X_test = X[90:99] # testing data\n",
    "\n",
    "#4. Normalizing the data\n",
    "X_train = normalize(X_train, norm='l2')\n",
    "X_train = X_train.T\n",
    "X_test = normalize(X_test, norm='l2')\n",
    "X_test = X_test.T\n",
    "\n",
    "\n",
    "#5. Extract the output from the dataset\n",
    "y = iris[\"target\"]\n",
    "y_train=y[0:89]\n",
    "y_test = y[90:99] \n",
    "\n",
    "#6. Reshaping the output\n",
    "y_train=np.reshape(y_train, (1, 89)) #\n",
    "y_train=y_train.T\n",
    "y_test = np.reshape(y_test, (1, 9))\n",
    "y_test = y_test.T\n",
    "\n",
    "# 7. Normalizing the output\n",
    "y_train = normalize(y_train, norm='l2')\n",
    "y_test = normalize(y_test, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d86d3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Adding bias to the model\n",
    "ones = np.ones((1, 89))\n",
    "X_train_b1 = np.append(ones, X_train, axis=0)\n",
    "X_train_b1 = X_train_b1.T\n",
    "zeros = np.ones((1,9))\n",
    "X_test_b1 = np.append(zeros, X_test, axis=0).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be53079",
   "metadata": {},
   "source": [
    "## Computing Errors For the Following iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bbeebd",
   "metadata": {},
   "source": [
    "#### Error After 10 iterations , this is just a little effort to compute the efficient weights for prediction\n",
    "* Error is quite high "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3029812a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5617977528089888"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = SoftmaxRegression(epochs=10)\n",
    "ola.train(X_train_b1,y_train)\n",
    "y_pred = ola.predict(X_train_b1)\n",
    "ola.predict_proba(y_pred)\n",
    "ola.error(y_pred,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d165dd",
   "metadata": {},
   "source": [
    "#### Error After 100 iterations \n",
    "* Error reduces but stilll quite high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7029896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43820224719101125"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = SoftmaxRegression(epochs=100)\n",
    "ola.train(X_train_b1,y_train)\n",
    "y_pred = ola.predict(X_train_b1)\n",
    "ola.predict_proba(y_pred)\n",
    "ola.error(y_pred,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64424f",
   "metadata": {},
   "source": [
    "#### Error After 1000 iterations \n",
    "\n",
    "* Error is 0  , that is , the model is predicting exactly the same as the test\n",
    "* Observation : Error is 0 , this means the training is overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4adb19c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = SoftmaxRegression(epochs=1000)\n",
    "ola.train(X_train_b1,y_train)\n",
    "y_pred = ola.predict(X_train_b1)\n",
    "ola.predict_proba(y_pred)\n",
    "ola.error(y_pred,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c7a4cd",
   "metadata": {},
   "source": [
    "#### Error After 10000 iterations \n",
    "* Error is 0  , that is , the model is predicting exactly the same as the test\n",
    "* The gradient descent has reached its optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb04bb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ola = SoftmaxRegression(epochs=10000)\n",
    "ola.train(X_train_b1,y_train)\n",
    "y_pred = ola.predict(X_t_b1)\n",
    "ola.predict_proba(y_pred)\n",
    "ola.error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d9b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd6dace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7faf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
